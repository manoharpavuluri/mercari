{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordbatch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d11e96c8e6b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m###until Kaggle admins fix the wordbatch pip package installation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m###sys.path.insert(0, '../input/wordbatch/wordbatch/')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwordbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordBag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordHash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordbatch'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "import sys\n",
    "\n",
    "###Add https://www.kaggle.com/anttip/wordbatch to your kernel Data Sources, \n",
    "###until Kaggle admins fix the wordbatch pip package installation\n",
    "###sys.path.insert(0, '../input/wordbatch/wordbatch/')\n",
    "import wordbatch\n",
    "\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL, FM_FTRL\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "NUM_BRANDS = 5300\n",
    "NUM_CATEGORIES = 1290\n",
    "\n",
    "develop = False\n",
    "# develop= True\n",
    "\n",
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y0), 2)))\n",
    "\n",
    "\n",
    "def split_cat(text):\n",
    "    try:\n",
    "        return text.split(\"/\")\n",
    "    except:\n",
    "        return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['general_cat'].fillna(value='missing', inplace=True)\n",
    "    dataset['subcat_1'].fillna(value='missing', inplace=True)\n",
    "    dataset['subcat_2'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'missing'\n",
    "    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'missing'\n",
    "    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'missing'\n",
    "\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['general_cat'] = dataset['general_cat'].astype('category')\n",
    "    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n",
    "    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "\n",
    "\n",
    "# Define helpers for text normalization\n",
    "stopwords = {x: 1 for x in stopwords.words('english')}\n",
    "non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return u\" \".join(\n",
    "        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n",
    "         if len(x) > 1 and x not in stopwords])\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    from time import gmtime, strftime\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "\n",
    "    # if 1 == 1:\n",
    "    ###train = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv', engine='c')\n",
    "    ###test = pd.read_table('../input/mercari-price-suggestion-challenge/test.tsv', engine='c')\n",
    "\n",
    "    train = pd.read_table('/Users/dad/Downloads/mercari-price-suggestion-challenge/train.tsv', engine='c')\n",
    "    test = pd.read_table('/Users/dad/Downloads/mercari-price-suggestion-challenge/test.tsv', engine='c')\n",
    "\n",
    "    print('[{}] Finished to load data'.format(time.time() - start_time))\n",
    "    print('Train shape: ', train.shape)\n",
    "    print('Test shape: ', test.shape)\n",
    "    nrow_test = train.shape[0]  # -dftt.shape[0]\n",
    "    dftt = train[(train.price < 1.0)]\n",
    "    train = train.drop(train[(train.price < 1.0)].index)\n",
    "    del dftt['price']\n",
    "    nrow_train = train.shape[0]\n",
    "    # print(nrow_train, nrow_test)\n",
    "    y = np.log1p(train[\"price\"])\n",
    "    merge: pd.DataFrame = pd.concat([train, dftt, test])\n",
    "    submission: pd.DataFrame = test[['test_id']]\n",
    "\n",
    "    del train\n",
    "    del test\n",
    "    gc.collect()\n",
    "\n",
    "    merge['general_cat'], merge['subcat_1'], merge['subcat_2'] = \\\n",
    "        zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n",
    "    merge.drop('category_name', axis=1, inplace=True)\n",
    "    print('[{}] Split categories completed.'.format(time.time() - start_time))\n",
    "\n",
    "    handle_missing_inplace(merge)\n",
    "    print('[{}] Handle missing completed.'.format(time.time() - start_time))\n",
    "\n",
    "    cutting(merge)\n",
    "    print('[{}] Cut completed.'.format(time.time() - start_time))\n",
    "\n",
    "    to_categorical(merge)\n",
    "    print('[{}] Convert categorical completed'.format(time.time() - start_time))\n",
    "\n",
    "    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0],\n",
    "                                                                  \"hash_size\": 2 ** 29, \"norm\": None, \"tf\": 'binary',\n",
    "                                                                  \"idf\": None,\n",
    "                                                                  }), procs=8)\n",
    "    wb.dictionary_freeze= True\n",
    "    X_name = wb.fit_transform(merge['name'])\n",
    "    del(wb)\n",
    "    X_name = X_name[:, np.array(np.clip(X_name.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n",
    "    print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    wb = CountVectorizer()\n",
    "    X_category1 = wb.fit_transform(merge['general_cat'])\n",
    "    X_category2 = wb.fit_transform(merge['subcat_1'])\n",
    "    X_category3 = wb.fit_transform(merge['subcat_2'])\n",
    "    print('[{}] Count vectorize `categories` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    # wb= wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 3, \"hash_ngrams_weights\": [1.0, 1.0, 0.5],\n",
    "    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                                                                  \"hash_size\": 2 ** 29, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                                                  \"idf\": None})\n",
    "                             , procs=8)\n",
    "    wb.dictionary_freeze= True\n",
    "    X_description = wb.fit_transform(merge['item_description'])\n",
    "    del(wb)\n",
    "    X_description = X_description[:, np.array(np.clip(X_description.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n",
    "    print('[{}] Vectorize `item_description` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    lb = LabelBinarizer(sparse_output=True)\n",
    "    X_brand = lb.fit_transform(merge['brand_name'])\n",
    "    print('[{}] Label binarize `brand_name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n",
    "                                          sparse=True).values)\n",
    "    print('[{}] Get dummies on `item_condition_id` and `shipping` completed.'.format(time.time() - start_time))\n",
    "    print(X_dummies.shape, X_description.shape, X_brand.shape, X_category1.shape, X_category2.shape, X_category3.shape,\n",
    "          X_name.shape)\n",
    "    sparse_merge = hstack((X_dummies, X_description, X_brand, X_category1, X_category2, X_category3, X_name)).tocsr()\n",
    "\n",
    "    print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n",
    "\n",
    "    #    pd.to_pickle((sparse_merge, y), \"xy.pkl\")\n",
    "    # else:\n",
    "    #    nrow_train, nrow_test= 1481661, 1482535\n",
    "    #    sparse_merge, y = pd.read_pickle(\"xy.pkl\")\n",
    "\n",
    "    # Remove features with document frequency <=1\n",
    "    print(sparse_merge.shape)\n",
    "    mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 1, 0, 1), dtype=bool)\n",
    "    sparse_merge = sparse_merge[:, mask]\n",
    "    X = sparse_merge[:nrow_train]\n",
    "    X_test = sparse_merge[nrow_test:]\n",
    "    print(sparse_merge.shape)\n",
    "\n",
    "    gc.collect()\n",
    "    train_X, train_y = X, y\n",
    "    if develop:\n",
    "        train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.05, random_state=100)\n",
    "\n",
    "    model = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=sparse_merge.shape[1], iters=47, inv_link=\"identity\", threads=1)\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    print('[{}] Train FTRL completed'.format(time.time() - start_time))\n",
    "    if develop:\n",
    "        preds = model.predict(X=valid_X)\n",
    "        print(\"FTRL dev RMSLE:\", rmsle(np.expm1(valid_y), np.expm1(preds)))\n",
    "\n",
    "    predsF = model.predict(X_test)\n",
    "    print('[{}] Predict FTRL completed'.format(time.time() - start_time))\n",
    "\n",
    "    model = FM_FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=0.1, D=sparse_merge.shape[1], alpha_fm=0.01, L2_fm=0.0, init_fm=0.01,\n",
    "                    D_fm=200, e_noise=0.0001, iters=18, inv_link=\"identity\", threads=4)\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    print('[{}] Train ridge v2 completed'.format(time.time() - start_time))\n",
    "    if develop:\n",
    "        preds = model.predict(X=valid_X)\n",
    "        print(\"FM_FTRL dev RMSLE:\", rmsle(np.expm1(valid_y), np.expm1(preds)))\n",
    "\n",
    "    predsFM = model.predict(X_test)\n",
    "    print('[{}] Predict FM_FTRL completed'.format(time.time() - start_time))\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.57,\n",
    "        'application': 'regression',\n",
    "        'max_depth': 5,\n",
    "        'num_leaves': 32,\n",
    "        'verbosity': -1,\n",
    "        'metric': 'RMSE',\n",
    "        'data_random_seed': 1,\n",
    "        'bagging_fraction': 0.6,\n",
    "        'bagging_freq': 5,\n",
    "        'feature_fraction': 0.65,\n",
    "        'nthread': 4,\n",
    "        'min_data_in_leaf': 100,\n",
    "        'max_bin': 31\n",
    "    }\n",
    "\n",
    "    # Remove features with document frequency <=100\n",
    "    print(sparse_merge.shape)\n",
    "    mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 100, 0, 1), dtype=bool)\n",
    "    sparse_merge = sparse_merge[:, mask]\n",
    "    X = sparse_merge[:nrow_train]\n",
    "    X_test = sparse_merge[nrow_test:]\n",
    "    print(sparse_merge.shape)\n",
    "\n",
    "    train_X, train_y = X, y\n",
    "    if develop:\n",
    "        train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.05, random_state=100)\n",
    "\n",
    "    d_train = lgb.Dataset(train_X, label=train_y)\n",
    "    watchlist = [d_train]\n",
    "    if develop:\n",
    "        d_valid = lgb.Dataset(valid_X, label=valid_y)\n",
    "        watchlist = [d_train, d_valid]\n",
    "\n",
    "    model = lgb.train(params, train_set=d_train, num_boost_round=5500, valid_sets=watchlist, \\\n",
    "                      early_stopping_rounds=1000, verbose_eval=1000)\n",
    "\n",
    "    if develop:\n",
    "        preds = model.predict(valid_X)\n",
    "        print(\"LGB dev RMSLE:\", rmsle(np.expm1(valid_y), np.expm1(preds)))\n",
    "\n",
    "    predsL = model.predict(X_test)\n",
    "\n",
    "    print('[{}] Predict LGB completed.'.format(time.time() - start_time))\n",
    "\n",
    "    preds = (predsF * 0.1 + predsL * 0.22 + predsFM * 0.68)\n",
    "\n",
    "    submission['price'] = np.expm1(preds)\n",
    "    submission.to_csv(\"submission_wordbatch_ftrl_fm_lgb.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
